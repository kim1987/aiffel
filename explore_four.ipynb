{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from In [73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "file_path = os.path.join(os.getcwd(),'lyricist/data/shakespeare.txt')\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()\n",
    "\n",
    "print(raw_corpus[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    if idx > 9: break\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence. <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\",r\"\\1\",sentence)\n",
    "    sentence = re.sub(r'[\" \"]+',\" \",sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z.!,¿]+\",\" \",sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further, hear me speak. <end>',\n",
       " '<start> speak, speak. <end>',\n",
       " '<start> you are all resolved rather to die than to famish <end>',\n",
       " '<start> resolved. resolved. <end>',\n",
       " '<start> first, you know caius marcius is chief enemy to the people. <end>',\n",
       " '<start> we know t, we know t. <end>',\n",
       " '<start> let us kill him, and we ll have corn at our own price. <end>',\n",
       " '<start> is t a verdict <end>',\n",
       " '<start> no more talking on t let it be done away, away! <end>',\n",
       " '<start> one word, good citizens. <end>']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  144   35 ...    0    0    0]\n",
      " [   2  534  497 ...    0    0    0]\n",
      " [   2   11   42 ...    0    0    0]\n",
      " ...\n",
      " [   2  138    1 ...    0    0    0]\n",
      " [   2   28   56 ...    0    0    0]\n",
      " [   2 1003   28 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f832e4e0400>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      num_words=7000,\n",
    "      filters=' ',\n",
    "      oov_token=\"<unk>\"\n",
    "    )\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.\\\n",
    "     sequence.pad_sequences(tensor,padding='post')\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor,tokenizer   \n",
    "\n",
    "tensor,tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  144   35 1334  129 3556  125   29  497    3]\n",
      " [   2  534  497    3    0    0    0    0    0    0]\n",
      " [   2   11   42   41 1549  307    7  277   61    7]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : the\n",
      "5 : and\n",
      "6 : i\n",
      "7 : to\n",
      "8 : of\n",
      "9 : my\n",
      "10 : a\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx >=10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2  144   35 1334  129 3556  125   29  497    3    0    0    0    0\n",
      "    0    0    0]\n",
      "[ 144   35 1334  129 3556  125   29  497    3    0    0    0    0    0\n",
      "    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:,:-1]\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 17), (256, 17)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input,tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_size,hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size,return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size,return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "\n",
    "model = TextGenerator(tokenizer.num_words + 1,\n",
    "                       embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 17, 7001), dtype=float32, numpy=\n",
       "array([[[ 6.79663062e-05, -6.87889406e-05,  3.01865453e-04, ...,\n",
       "         -1.96520952e-04,  3.05562833e-04, -1.38986696e-04],\n",
       "        [ 1.47487139e-04, -2.74763297e-04,  3.97127820e-04, ...,\n",
       "         -4.92529245e-04,  4.65255696e-04, -7.42471748e-05],\n",
       "        [ 1.50080770e-04, -3.73506366e-04,  5.31232508e-04, ...,\n",
       "         -7.89165206e-04,  5.24469011e-04, -1.94663895e-04],\n",
       "        ...,\n",
       "        [-8.54677521e-04,  3.96192120e-03, -1.09323370e-03, ...,\n",
       "          1.77032792e-03,  3.07216142e-05, -1.46165746e-03],\n",
       "        [-8.69188807e-04,  4.50208643e-03, -1.56830938e-03, ...,\n",
       "          1.87329971e-03, -1.62460681e-04, -1.54612714e-03],\n",
       "        [-8.44255497e-04,  4.97404719e-03, -2.02514371e-03, ...,\n",
       "          1.93956820e-03, -3.51499446e-04, -1.60801236e-03]],\n",
       "\n",
       "       [[ 6.79663062e-05, -6.87889406e-05,  3.01865453e-04, ...,\n",
       "         -1.96520952e-04,  3.05562833e-04, -1.38986696e-04],\n",
       "        [-3.50908267e-05,  1.15211209e-04,  1.44904072e-04, ...,\n",
       "         -1.50273714e-04,  5.98523533e-04,  2.84673297e-05],\n",
       "        [-2.48655328e-04,  5.59652981e-05, -9.53026210e-06, ...,\n",
       "          2.99303938e-05,  8.07749282e-04,  4.66439524e-04],\n",
       "        ...,\n",
       "        [-6.24099630e-04,  3.16488463e-03, -1.69474899e-03, ...,\n",
       "          8.97943974e-04,  5.47779433e-04, -1.17237156e-03],\n",
       "        [-6.71719143e-04,  3.81485093e-03, -1.93613302e-03, ...,\n",
       "          1.04657863e-03,  3.83769860e-04, -1.30918750e-03],\n",
       "        [-6.74080511e-04,  4.38497635e-03, -2.21497449e-03, ...,\n",
       "          1.17457670e-03,  1.97523914e-04, -1.41771883e-03]],\n",
       "\n",
       "       [[ 6.79663062e-05, -6.87889406e-05,  3.01865453e-04, ...,\n",
       "         -1.96520952e-04,  3.05562833e-04, -1.38986696e-04],\n",
       "        [ 2.76044448e-04, -8.30902281e-05,  4.37191105e-04, ...,\n",
       "         -3.14374687e-04,  1.30813438e-04,  9.79246761e-05],\n",
       "        [ 1.31130437e-04, -2.63921975e-04,  1.51024477e-04, ...,\n",
       "         -3.76569136e-04, -3.96813644e-04,  2.60449277e-04],\n",
       "        ...,\n",
       "        [-1.53604860e-03,  3.03911581e-03, -1.76003773e-03, ...,\n",
       "          3.85233172e-04, -1.04104671e-04, -1.43757579e-03],\n",
       "        [-1.61287154e-03,  3.66109447e-03, -2.02931813e-03, ...,\n",
       "          6.36052690e-04, -1.09189554e-04, -1.48235960e-03],\n",
       "        [-1.60910364e-03,  4.22411039e-03, -2.32961262e-03, ...,\n",
       "          8.50365963e-04, -1.63135162e-04, -1.52629404e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 6.79663062e-05, -6.87889406e-05,  3.01865453e-04, ...,\n",
       "         -1.96520952e-04,  3.05562833e-04, -1.38986696e-04],\n",
       "        [-2.63217662e-05, -2.50427169e-04,  7.01453362e-04, ...,\n",
       "         -2.29682788e-04,  2.95454112e-04, -3.47900728e-04],\n",
       "        [-1.98638678e-04,  4.07485386e-05,  4.00345918e-04, ...,\n",
       "         -3.95591313e-04,  1.29464839e-04, -5.10701851e-04],\n",
       "        ...,\n",
       "        [-1.91684556e-03,  3.05540184e-03, -9.01101739e-04, ...,\n",
       "          4.09485307e-04, -2.48888828e-04, -1.84565166e-03],\n",
       "        [-1.92025502e-03,  3.70698958e-03, -1.30502239e-03, ...,\n",
       "          6.59310783e-04, -3.30842682e-04, -1.85484975e-03],\n",
       "        [-1.85476150e-03,  4.30099433e-03, -1.72067876e-03, ...,\n",
       "          8.79423402e-04, -4.36130009e-04, -1.84865086e-03]],\n",
       "\n",
       "       [[ 6.79663062e-05, -6.87889406e-05,  3.01865453e-04, ...,\n",
       "         -1.96520952e-04,  3.05562833e-04, -1.38986696e-04],\n",
       "        [ 2.98861036e-04, -1.07233005e-04,  4.60168347e-04, ...,\n",
       "         -5.49075077e-04,  3.22567357e-04, -2.54357496e-04],\n",
       "        [ 1.61938311e-04, -4.39426600e-04,  5.17301669e-04, ...,\n",
       "         -6.74479990e-04,  3.20614316e-04, -2.01923453e-04],\n",
       "        ...,\n",
       "        [-4.77339199e-04,  1.38477667e-03, -1.23317190e-03, ...,\n",
       "          2.63069407e-04, -5.62220870e-04, -1.50639913e-03],\n",
       "        [-6.35373872e-04,  2.16479716e-03, -1.45400444e-03, ...,\n",
       "          4.51061642e-04, -4.74489207e-04, -1.63956627e-03],\n",
       "        [-7.53122091e-04,  2.89238011e-03, -1.72974856e-03, ...,\n",
       "          6.34198193e-04, -4.57186747e-04, -1.74245832e-03]],\n",
       "\n",
       "       [[ 6.79663062e-05, -6.87889406e-05,  3.01865453e-04, ...,\n",
       "         -1.96520952e-04,  3.05562833e-04, -1.38986696e-04],\n",
       "        [ 1.30280634e-04,  3.14480130e-04,  3.57868383e-04, ...,\n",
       "         -2.67755648e-04,  7.09812797e-04, -5.36058214e-04],\n",
       "        [ 1.00797799e-04,  3.87792767e-04,  3.78400233e-04, ...,\n",
       "          3.43511711e-05,  7.00135366e-04, -6.52326329e-04],\n",
       "        ...,\n",
       "        [-4.77067224e-04,  2.36821547e-03, -9.49457462e-04, ...,\n",
       "          1.20050786e-03, -6.91134483e-04, -1.85088743e-03],\n",
       "        [-6.17810059e-04,  3.03779077e-03, -1.20073941e-03, ...,\n",
       "          1.30702625e-03, -7.05422543e-04, -1.92683097e-03],\n",
       "        [-7.03523343e-04,  3.66598275e-03, -1.51245098e-03, ...,\n",
       "          1.39861845e-03, -7.40622287e-04, -1.97721040e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1):break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-05 20:41:48.353062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-05 20:41:48.353904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-05 20:41:48.354606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-05 20:41:48.355401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-05 20:41:48.356073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-05 20:41:48.356635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 9"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "427 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-05 20:41:52.001494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-05 20:41:52.002338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-05 20:41:52.003051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-05 20:41:52.003861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-05 20:41:52.004560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-05 20:41:52.005132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 9427 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 3122075171146588563,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 9885384704\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 6153058489920948743\n",
       " physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-05 20:30:35.316422: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 6s 47ms/step - loss: 3.6143\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 4s 47ms/step - loss: 2.8686\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 4s 47ms/step - loss: 2.7884\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 4s 47ms/step - loss: 2.7299\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 4s 47ms/step - loss: 2.6816\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 4s 47ms/step - loss: 2.6354\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 4s 47ms/step - loss: 2.5754\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 5s 49ms/step - loss: 2.5174\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 5s 49ms/step - loss: 2.4643\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 5s 49ms/step - loss: 2.4186\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 5s 49ms/step - loss: 2.3691\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 5s 48ms/step - loss: 2.3215\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 4s 48ms/step - loss: 2.2725\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 5s 50ms/step - loss: 2.2210\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 5s 48ms/step - loss: 2.1698\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 4s 48ms/step - loss: 2.1190\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 5s 49ms/step - loss: 2.0664: 0s - loss: - ETA: 0s - loss: \n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 5s 49ms/step - loss: 2.0128\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 5s 49ms/step - loss: 1.9616\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 5s 49ms/step - loss: 1.9076\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 4s 47ms/step - loss: 1.8540\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 4s 48ms/step - loss: 1.7984\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 5s 49ms/step - loss: 1.7439\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 5s 49ms/step - loss: 1.6891\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 4s 48ms/step - loss: 1.6336\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 5s 49ms/step - loss: 1.5773\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 4s 48ms/step - loss: 1.5220\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 4s 47ms/step - loss: 1.4671\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 5s 49ms/step - loss: 1.4139\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 5s 50ms/step - loss: 1.3593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f832e4a0220>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "  from_logits=True,\n",
    "  reduction='none'\n",
    ")\n",
    "model.compile(loss=loss,optimizer=optimizer)\n",
    "model.fit(dataset,epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer,init_sentence=\"<start>\", max_len=20):\n",
    "    \n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input,dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        predict = model(test_tensor)\n",
    "        \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict,axis=-1),axis=-1)[:,-1]\n",
    "        \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word,axis=0)],axis=-1)\n",
    "        \n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "    \n",
    "    generated=\"\"\n",
    "    \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \"\"\n",
    "    return generated\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start>he<end>tedious,tiebutcherschartercharterstarve,grave,pembroke,pembroke,dozendozendozendozenpaulina,paulina,ay,ay,'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he <end>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=500):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he tedious, meet. meet. meat, days! abuse abuse were, were, were, were, grandfather art, art, shepherds purchase purchase purchase entreat, accident entreat, entreat, entreat, taken vineyard vineyard flatterer, flatterer, honour! whilst whilst mankind mankind mankind whoever whoever whoever whoever whoever whoever whoever fathom fathom fathom fathom slander slander slander slander preposterous preposterous madam madam madam perceive perceive perceive house. battle battle battle battle battle commonwealth a a mantua mantua mantua because because mantua mantua mantua do heaven! heaven! majesty. majesty. majesty. hanged hanged hanged native native native hollow hollow disposition, followers followers followers midnight. ports ports neighbour, neighbour, savage savage presently. service presently. harm harm bottled bottled folly. folly. folly. harm harm folly. maid! maid! maid! rob rob hangs rob rob hangs strike, strike, strike, strike, head head head treaty treaty commonwealth milder milder milder contains preposterous preposterous preposterous slave, slave, dearth air, guess, arm commune knees knees nest lancaster! bleeding bleeding bleeding bleeding food bade bade bade deserving deserving deserving deserving brook brook brook brook brook court daughters daughters insult insult insult peace. ulysses otherwise, nettle gavest gavest gavest slipp slipp dreams, dreams, companion companion descent, descent, glory, glory, work blunt, foreign foreign foreign foreign shoulder shoulder regiment regiment another another herald herald desperate desperate desperate ended, ended, swear, swear, swear, villain, villain, wit studied studied studied studied studied play, play, play, play, knave knave knave knave knave impatient impatient impatient impatient maid maid pot, maid mab mab dove dove dove dove tend fashion, fashion, alike alike alike exton, sin! sin! enemy, intend tender true true boon. boon. boon. pomp, pomp, youthful youthful ask. ask. eager cloven neck. neck. corners fiend, bleeding bleeding bleeding hills changeling changeling changeling changeling changeling spoken, spoken, spoken, changeling long. long. long. long. lodging lodging lodging lodging work! work! work! anon anon anon anon anon anon shrew cave cave cave cave exactly exactly exactly nay, nay, nay, understand, understand, understand, born, born, born, truly truly perdita, perdita, perdita, stern, stern, bruised lancaster, royalty, royalty, royalty, royalty, graced graced graced what! what! what! what! room room could could could could could could could could left left left waiting waiting waiting waiting left waiting left left waiting waiting soft soft soft soft soft sin sin prompt prompt prompt its its glory. wert, elders. figure figure wert, wert, dealings dealings dispute dispute stones print print print print print saint saint saint saint saint saint saint out out out out out out out distinctly save save ways, dukedom, thief thief thief o o deceit deceit deceit vault vault then, then, then, morrow am. am. rings rings rings paris paris paris paris paris combine me, ran ran ran ran ran thoughts, thoughts, thoughts, thoughts, while. attempt attempt swell swell foe, foe, private life, private heal heal heal heal heal down, down, down, prank prank prank fruitful fruitful conqueror conqueror wild wild wild wild captives captives captives captives unknown unknown serious wander quarrel, quarrel, quarrel, quarrel, quarrel, quarrel, kind kind kind cockle entertainment, obey obey saint saint saint saint '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "txt_file_path = \"./lyricist/data/lyrics/*\"\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open (txt_file, 'r') as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    if len(re.findall(r'\\w+', sentence)) >= 15:\n",
    "        return \"\"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    \n",
    "    \n",
    "    tokenizer= tf.keras.preprocessing.text.Tokenizer(\n",
    "      num_words=12000,\n",
    "      filters=' ',\n",
    "      oov_token=\"<unk>\")\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=15,padding='post')\n",
    "\n",
    "    return tensor,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sen_to_corp(sentence):\n",
    "    \n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    for sentence in raw_corpus:\n",
    "        if len(sentence) == 0: continue\n",
    "            \n",
    "        preprocessed_sentence = preprocess_sentence(sentence)\n",
    "        \n",
    "        if len(preprocessed_sentence) == 0: continue\n",
    "            \n",
    "        corpus.append(preprocessed_sentence)\n",
    "        \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = sen_to_corp(sentence)\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "src_input = tensor[:,:-1]\n",
    "tgt_input = tensor[:,1:]\n",
    "enc_train, enc_val,dec_train, dec_val = train_test_split(src_input,tgt_input, test_size = 0.2, random_state = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124444, 14)\n",
      "Target Train: (124444, 14)\n",
      "[  2  34   5  24 124 202  10  45  44  60 536   3   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "print(src_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "486/486 [==============================] - 33s 60ms/step - loss: 3.4530\n",
      "Epoch 2/30\n",
      "486/486 [==============================] - 30s 60ms/step - loss: 2.9823\n",
      "Epoch 3/30\n",
      "486/486 [==============================] - 29s 60ms/step - loss: 2.8053\n",
      "Epoch 4/30\n",
      "486/486 [==============================] - 30s 60ms/step - loss: 2.6711\n",
      "Epoch 5/30\n",
      "486/486 [==============================] - 30s 61ms/step - loss: 2.5562\n",
      "Epoch 6/30\n",
      "486/486 [==============================] - 30s 61ms/step - loss: 2.4508\n",
      "Epoch 7/30\n",
      "486/486 [==============================] - 30s 61ms/step - loss: 2.3519\n",
      "Epoch 8/30\n",
      "486/486 [==============================] - 30s 61ms/step - loss: 2.2581\n",
      "Epoch 9/30\n",
      "486/486 [==============================] - 30s 61ms/step - loss: 2.1687\n",
      "Epoch 10/30\n",
      "486/486 [==============================] - 30s 61ms/step - loss: 2.0836\n",
      "Epoch 11/30\n",
      "486/486 [==============================] - 30s 61ms/step - loss: 2.0029\n",
      "Epoch 12/30\n",
      "486/486 [==============================] - 29s 60ms/step - loss: 1.9255\n",
      "Epoch 13/30\n",
      "486/486 [==============================] - 30s 61ms/step - loss: 1.8516\n",
      "Epoch 14/30\n",
      "486/486 [==============================] - 31s 63ms/step - loss: 1.7801\n",
      "Epoch 15/30\n",
      "486/486 [==============================] - 30s 62ms/step - loss: 1.7105\n",
      "Epoch 16/30\n",
      "486/486 [==============================] - 29s 59ms/step - loss: 1.6441\n",
      "Epoch 17/30\n",
      "486/486 [==============================] - 29s 59ms/step - loss: 1.5796\n",
      "Epoch 18/30\n",
      "486/486 [==============================] - 30s 62ms/step - loss: 1.5188\n",
      "Epoch 19/30\n",
      "486/486 [==============================] - 29s 59ms/step - loss: 1.4609\n",
      "Epoch 20/30\n",
      "486/486 [==============================] - 29s 59ms/step - loss: 1.4056\n",
      "Epoch 21/30\n",
      "486/486 [==============================] - 29s 59ms/step - loss: 1.3534\n",
      "Epoch 22/30\n",
      "486/486 [==============================] - 29s 59ms/step - loss: 1.3051\n",
      "Epoch 23/30\n",
      "486/486 [==============================] - 29s 59ms/step - loss: 1.2598\n",
      "Epoch 24/30\n",
      "486/486 [==============================] - 29s 59ms/step - loss: 1.2176\n",
      "Epoch 25/30\n",
      "486/486 [==============================] - 30s 60ms/step - loss: 1.1789\n",
      "Epoch 26/30\n",
      "486/486 [==============================] - 29s 59ms/step - loss: 1.1435\n",
      "Epoch 27/30\n",
      "486/486 [==============================] - 29s 60ms/step - loss: 1.1113\n",
      "Epoch 28/30\n",
      "486/486 [==============================] - 29s 60ms/step - loss: 1.0820\n",
      "Epoch 29/30\n",
      "486/486 [==============================] - 29s 59ms/step - loss: 1.0562\n",
      "Epoch 30/30\n",
      "486/486 [==============================] - 29s 60ms/step - loss: 1.0319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f25cba172b0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    \n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "\n",
    "    while True:\n",
    "\n",
    "        predict = model(test_tensor) \n",
    "\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> adam and eve moves like a fist through traffic <end> '"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> adam\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
