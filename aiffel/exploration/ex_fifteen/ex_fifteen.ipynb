{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex_fifteen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1s5QRAFbVU779eX_vrq5J5fkWfD_HSNar",
      "authorship_tag": "ABX9TyNFexhMlYnl5t3hEBJ1BOb5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kim1987/aiffel/blob/main/aiffel/exploration/ex_fifteen/ex_fifteen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io8SsicJQoa8"
      },
      "source": [
        "exploration_15  \n",
        "with pytorch transformer\n",
        "and https://tutorials.pytorch.kr/beginner/translation_transformer.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJhQ6js8RXyV"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "735DPq5AYqHq"
      },
      "source": [
        "import copy\n",
        "from typing import Optional, Any, Union, Callable, Tuple\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.nn import functional as F\n",
        "#from torch.nn.module.module import Module\n",
        "#from torch.nn.module.activation import MultiheadAttention\n",
        "from torch.nn.modules.container import ModuleList\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn.modules.dropout import Dropout\n",
        "from torch.nn.modules.linear import Linear\n",
        "from torch.nn.modules.normalization import LayerNorm\n",
        "from torch.nn.parameter import Parameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APgonupsYqLm"
      },
      "source": [
        "if torch.cuda.is_available():  \n",
        "  device = torch.device(\"cuda:0\") \n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpnXEpFkRYGr"
      },
      "source": [
        "make model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CV79dPLQcFJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "d94ff2e3-dab0-499a-a878-8e933c4e2820"
      },
      "source": [
        "'''\n",
        "class PositionalEncoding(nn.Module): # in https://github.com/pytorch/examples/blob/master/word_language_model/model.py\n",
        "\n",
        "  def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "    super().__init__()\n",
        "    self.dropout = Dropout(p=dropout)\n",
        "    pe = torch.zeros(max_len, d_model) #Tensor type\n",
        "    position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)# (maxlen,1) word position.\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))# 10000^2i/d_model\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0).transpose(0, 1) #(max_len,1,d_model)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "      \"\"\"\n",
        "      Shape:\n",
        "          x: [sequence length, batch size, embed dim]\n",
        "          output: [sequence length, batch size, embed dim]\n",
        "      \"\"\"\n",
        "      \n",
        "      x = x + self.pe[:x.size(0), :]\n",
        "      return self.dropout(x)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nclass PositionalEncoding(nn.Module): # in https://github.com/pytorch/examples/blob/master/word_language_model/model.py\\n\\n  def __init__(self, d_model, dropout=0.1, max_len=5000):\\n    super().__init__()\\n    self.dropout = Dropout(p=dropout)\\n    pe = torch.zeros(max_len, d_model) #Tensor type\\n    position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)# (maxlen,1) word position.\\n    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))# 10000^2i/d_model\\n    pe[:, 0::2] = torch.sin(position * div_term)\\n    pe[:, 1::2] = torch.cos(position * div_term)\\n    pe = pe.unsqueeze(0).transpose(0, 1) #(max_len,1,d_model)\\n    self.register_buffer(\\'pe\\', pe)\\n\\n  def forward(self, x):\\n      \"\"\"\\n      Shape:\\n          x: [sequence length, batch size, embed dim]\\n          output: [sequence length, batch size, embed dim]\\n      \"\"\"\\n      \\n      x = x + self.pe[:x.size(0), :]\\n      return self.dropout(x)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoPkYnQROw8J"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 max_len: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, max_len).reshape(max_len, 1)\n",
        "        pos_embedding = torch.zeros((max_len, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        print(self.pos_embedding.shape, ' test',token_embedding.shape)\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_Ds1cxgQn1M"
      },
      "source": [
        "class TransformerModel(nn.Module): # in https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py \n",
        "                                   # in https://github.com/pytorch/examples/blob/master/word_language_model/model.py \n",
        "  \n",
        "    def __init__(self, d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,\n",
        "                 num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
        "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
        "                 layer_norm_eps: float = 1e-5, device=None, dtype=None,batch_first: bool = False,\n",
        "                 max_len:int = 5000):\n",
        "        '''\n",
        "        Args:\n",
        "            d_model: the number of expected features in the encoder/decoder inputs (default=512).\n",
        "            nhead: the number of heads in the multiheadattention models (default=8).\n",
        "            num_encoder_layers: the number of sub-encoder-layers in the encoder (default=6).\n",
        "            num_decoder_layers: the number of sub-decoder-layers in the decoder (default=6).\n",
        "            dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
        "            dropout: the dropout value (default=0.1).\n",
        "            activation: the activation function of encoder/decoder intermediate layer, can be a string\n",
        "                (\"relu\" or \"gelu\") or a unary callable. Default: relu\n",
        "            layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
        "                other attention and feedforward operations, otherwise after. Default: ``False`` (after).\n",
        "            batch_first: If ``True``, then the input and output tensors are provided\n",
        "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
        "        '''\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
        "                                                activation, layer_norm_eps,\n",
        "                                                **factory_kwargs)\n",
        "        encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
        "\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
        "                                                activation, layer_norm_eps,\n",
        "                                                **factory_kwargs)\n",
        "        decoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def forward(self, src: Tensor, tgt: Tensor, #src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None,\n",
        "              #memory_mask: Optional[Tensor] = None, \n",
        "              src_key_padding_mask: Optional[Tensor] = None,tgt_key_padding_mask: Optional[Tensor] = None, \n",
        "              memory_key_padding_mask: Optional[Tensor] = None, max_len:int = 40\n",
        "              ) -> Tensor:\n",
        "        \"\"\"Take in and process masked source/target sequences.\n",
        "        Args:\n",
        "            src: the sequence to the encoder (required).\n",
        "            tgt: the sequence to the decoder (required).\n",
        "            src_mask: the additive mask for the src sequence (optional).\n",
        "            tgt_mask: the additive mask for the tgt sequence (optional).\n",
        "            memory_mask: the additive mask for the encoder output (optional).\n",
        "            src_key_padding_mask: the ByteTensor mask for src keys per batch (optional).\n",
        "            tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional).\n",
        "            memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).\n",
        "        Shape:\n",
        "            - src: :math:`(S, N, E)`, `(N, S, E)` if batch_first.\n",
        "            - tgt: :math:`(T, N, E)`, `(N, T, E)` if batch_first.\n",
        "            - src_mask: :math:`(S, S)`.\n",
        "            - tgt_mask: :math:`(T, T)`.\n",
        "            - memory_mask: :math:`(T, S)`.\n",
        "            - src_key_padding_mask: :math:`(N, S)`.\n",
        "            - tgt_key_padding_mask: :math:`(N, T)`.\n",
        "            - memory_key_padding_mask: :math:`(N, S)`.\n",
        "            Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked\n",
        "            positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
        "            while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
        "            are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
        "            is provided, it will be added to the attention weight.\n",
        "            [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\n",
        "            the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero\n",
        "            positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
        "            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
        "            - output: :math:`(T, N, E)`, `(N, T, E)` if batch_first.\n",
        "            Note: Due to the multi-head attention architecture in the transformer model,\n",
        "            the output sequence length of a transformer is same as the input sequence\n",
        "            (i.e. target) length of the decode.\n",
        "            where S is the source sequence length, T is the target sequence length, N is the\n",
        "            batch size, E is the feature number\n",
        "        \"\"\"\n",
        "  \n",
        "        #포지셔닝, 마스크.\n",
        "\n",
        "        src_len = src.shape[0]\n",
        "        tgt_len = tgt.shape[0]\n",
        "\n",
        "        src_mask = self._generate_square_subsequent_mask(src_len,src_len)\n",
        "        tgt_mask = self._generate_square_subsequent_mask(tgt_len,src_len)\n",
        "        memory_mask = src_mask\n",
        "\n",
        "        if src_key_padding_mask is not None:\n",
        "          memory_key_padding_mask = src_key_padding_mask\n",
        "\n",
        "        if src.size(1) != tgt.size(1):\n",
        "          raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
        "        if src.size(2) != self.d_model or tgt.size(2) != self.d_model:\n",
        "          raise RuntimeError(\"the feature number of src and tgt must be equal to d_model\")\n",
        "\n",
        "\n",
        "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
        "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                              memory_key_padding_mask=memory_key_padding_mask)\n",
        "        return output\n",
        "        \n",
        "    @staticmethod\n",
        "    def _generate_square_subsequent_mask(tgt_len, src_len):\n",
        "        return torch.triu(torch.ones(tgt_len,src_len)== 1,diagonal=1) \n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n",
        "\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1: #왜 2차원 이상이여야 하는가?\n",
        "                torch.nn.init.xavier_uniform_(p)\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJJWd-hwQn4_"
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    __constants__ = ['norm'] # use when   'in' method     a in b \n",
        "\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)# 레이어 복사.\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "        output = src\n",
        "        \n",
        "        for mod in self.layers:\n",
        "            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqp57I-9Qn62"
      },
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    __constants__ = ['norm']\n",
        "\n",
        "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.layers = _get_clones(decoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "        output = tgt\n",
        "\n",
        "        for mod in self.layers:\n",
        "            output = mod(output, memory, tgt_mask=tgt_mask,\n",
        "                         memory_mask=memory_mask,\n",
        "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                         memory_key_padding_mask=memory_key_padding_mask)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90rfxBubQn8x"
      },
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    r\"\"\"\n",
        "    Args:\n",
        "        d_model: the number of expected features in the input (required).\n",
        "        nhead: the number of heads in the multiheadattention models (required).\n",
        "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        activation: the activation function of the intermediate layer, can be a string\n",
        "            (\"relu\" or \"gelu\") or a unary callable. Default: relu\n",
        "        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
        "        batch_first: If ``True``, then the input and output tensors are provided\n",
        "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
        "        norm_first: if ``True``, layer norm is done prior to attention and feedforward\n",
        "            operations, respectivaly. Otherwise it's done after. Default: ``False`` (after).\n",
        "    \"\"\"\n",
        "    __constants__ = ['norm_first']\n",
        "\n",
        "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
        "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
        "                 layer_norm_eps: float = 1e-5, norm_first: bool = False,\n",
        "                 device=None, dtype=None,batch_first:bool=None) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
        "                                            **factory_kwargs)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
        "        self.dropout = Dropout(dropout)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
        "\n",
        "        self.norm_first = norm_first\n",
        "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        # Legacy string support for activation function.\n",
        "        if isinstance(activation, str):\n",
        "            self.activation = _get_activation_fn(activation)\n",
        "        else:\n",
        "            self.activation = activation\n",
        "\n",
        "    def __setstate__(self, state): # set for loads?\n",
        "        if 'activation' not in state:\n",
        "            state['activation'] = F.relu\n",
        "        super().__setstate__(state)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "\n",
        "        x = src\n",
        "        if self.norm_first:\n",
        "            x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask)\n",
        "            x = x + self._ff_block(self.norm2(x))\n",
        "        else: # base\n",
        "            x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))\n",
        "            x = self.norm2(x + self._ff_block(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "    # self-attention block\n",
        "    def _sa_block(self, x: Tensor,\n",
        "                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
        "        x = self.self_attn(x, x, x,\n",
        "                           attn_mask=attn_mask,\n",
        "                           key_padding_mask=key_padding_mask,\n",
        "                           need_weights=False)[0]\n",
        "        return self.dropout1(x)\n",
        "\n",
        "    # feed forward block\n",
        "    def _ff_block(self, x: Tensor) -> Tensor:\n",
        "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
        "        return self.dropout2(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtXFrZKCQn-5"
      },
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    r\"\"\"\n",
        "        d_model: the number of expected features in the input (required).\n",
        "        nhead: the number of heads in the multiheadattention models (required).\n",
        "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        activation: the activation function of the intermediate layer, can be a string\n",
        "            (\"relu\" or \"gelu\") or a unary callable. Default: relu\n",
        "        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
        "        batch_first: If ``True``, then the input and output tensors are provided\n",
        "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
        "        norm_first: if ``True``, layer norm is done prior to self attention, multihead\n",
        "            attention and feedforward operations, respectivaly. Otherwise it's done after.\n",
        "            Default: ``False`` (after).\n",
        "    \"\"\"\n",
        "    __constants__ = ['batch_first', 'norm_first']\n",
        "\n",
        "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
        "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
        "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
        "                 device=None, dtype=None) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
        "                                            **factory_kwargs)\n",
        "        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
        "                                                 **factory_kwargs)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
        "        self.dropout = Dropout(dropout)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
        "\n",
        "        self.norm_first = norm_first\n",
        "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "        self.dropout3 = Dropout(dropout)\n",
        "\n",
        "        # Legacy string support for activation function.\n",
        "        if isinstance(activation, str):\n",
        "            self.activation = _get_activation_fn(activation)\n",
        "        else:\n",
        "            self.activation = activation\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        if 'activation' not in state:\n",
        "            state['activation'] = F.relu\n",
        "        super(TransformerDecoderLayer, self).__setstate__(state)\n",
        "\n",
        "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "\n",
        "        x = tgt\n",
        "        if self.norm_first:\n",
        "            x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)\n",
        "            x = x + self._mha_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask)\n",
        "            x = x + self._ff_block(self.norm3(x))\n",
        "        else:\n",
        "            x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))\n",
        "            x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))\n",
        "            x = self.norm3(x + self._ff_block(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "    # self-attention block\n",
        "    def _sa_block(self, x: Tensor,\n",
        "                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
        "        x = self.self_attn(x, x, x,\n",
        "                           attn_mask=attn_mask,\n",
        "                           key_padding_mask=key_padding_mask,\n",
        "                           need_weights=False)[0]\n",
        "        return self.dropout1(x)\n",
        "\n",
        "    # multihead attention block\n",
        "    def _mha_block(self, x: Tensor, mem: Tensor,\n",
        "                   attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
        "        x = self.multihead_attn(x, mem, mem,\n",
        "                                attn_mask=attn_mask,\n",
        "                                key_padding_mask=key_padding_mask,\n",
        "                                need_weights=False)[0]\n",
        "        return self.dropout2(x)\n",
        "\n",
        "    # feed forward block\n",
        "    def _ff_block(self, x: Tensor) -> Tensor:\n",
        "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
        "        return self.dropout3(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cXDP_TrQoA0"
      },
      "source": [
        "def _get_clones(module, N):\n",
        "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    elif activation == \"gelu\":\n",
        "        return F.gelu\n",
        "\n",
        "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX-NL0c1QoC3"
      },
      "source": [
        "class MultiheadAttention(nn.Module): # https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py\n",
        "#torch.nn.functional 에도 function 으로 멀티헤드어텐션이 있음. multi_head_attention_forward\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        embed_dim: Total dimension of the model.\n",
        "        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\n",
        "            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\n",
        "        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\n",
        "        bias: If specified, adds bias to input / output projection layers. Default: ``True``.\n",
        "        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\n",
        "        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\n",
        "            Default: ``False``.\n",
        "        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\n",
        "        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\n",
        "        batch_first: If ``True``, then the input and output tensors are provided -> 위의 트랜스포머 모델에서 배치파일 안쓰도록 변경했지만 배치파일의 사용 방법을 확인하고자 삭제하지않음.\n",
        "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
        "    \"\"\"\n",
        "    __constants__ = ['batch_first']\n",
        "    bias_k: Optional[torch.Tensor]\n",
        "    bias_v: Optional[torch.Tensor]\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,\n",
        "                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.batch_first = batch_first\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        #총 개수가 맞아야 함. 나머지가 있으면 안됨.\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        #초기화?\n",
        "        if self._qkv_same_embed_dim is False:\n",
        "            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))\n",
        "            self.k_proj_weight = tParameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))\n",
        "            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))\n",
        "            self.register_parameter('in_proj_weight', None)\n",
        "        else:\n",
        "            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))\n",
        "            self.register_parameter('q_proj_weight', None)\n",
        "            self.register_parameter('k_proj_weight', None)\n",
        "            self.register_parameter('v_proj_weight', None)\n",
        "\n",
        "        if bias:\n",
        "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\n",
        "        else:\n",
        "            self.register_parameter('in_proj_bias', None)\n",
        "        self.out_proj = Linear(embed_dim, embed_dim, bias=bias, **factory_kwargs)\n",
        "\n",
        "        if add_bias_kv:\n",
        "            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
        "            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
        "        else:\n",
        "            self.bias_k = self.bias_v = None\n",
        "\n",
        "        self.add_zero_attn = add_zero_attn\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        if self._qkv_same_embed_dim:\n",
        "            xavier_uniform_(self.in_proj_weight)\n",
        "        else:\n",
        "            torch.nn.init.xavier_uniform_(self.q_proj_weight)\n",
        "            torch.nn.init.xavier_uniform_(self.k_proj_weight)\n",
        "            torch.nn.init.xavier_uniform_(self.v_proj_weight)\n",
        "\n",
        "        if self.in_proj_bias is not None:\n",
        "            torch.nn.init.constant_(self.in_proj_bias, 0.)\n",
        "            torch.nn.init.constant_(self.out_proj.bias, 0.)\n",
        "        if self.bias_k is not None:\n",
        "            torch.nn.init.xavier_normal_(self.bias_k)\n",
        "        if self.bias_v is not None:\n",
        "            torch.nn.init.xavier_normal_(self.bias_v)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
        "        if '_qkv_same_embed_dim' not in state:\n",
        "            state['_qkv_same_embed_dim'] = True\n",
        "\n",
        "        super(MultiheadAttention, self).__setstate__(state)\n",
        "\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None,\n",
        "                need_weights: bool = True, attn_mask: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:\n",
        "        r\"\"\"\n",
        "    Args:\n",
        "        query: Query embeddings of shape :math:`(L, N, E_q)` when ``batch_first=False`` or :math:`(N, L, E_q)`\n",
        "            when ``batch_first=True``, where :math:`L` is the target sequence length, :math:`N` is the batch size,\n",
        "            and :math:`E_q` is the query embedding dimension ``embed_dim``. Queries are compared against\n",
        "            key-value pairs to produce the output. See \"Attention Is All You Need\" for more details.\n",
        "        key: Key embeddings of shape :math:`(S, N, E_k)` when ``batch_first=False`` or :math:`(N, S, E_k)` when\n",
        "            ``batch_first=True``, where :math:`S` is the source sequence length, :math:`N` is the batch size, and\n",
        "            :math:`E_k` is the key embedding dimension ``kdim``. See \"Attention Is All You Need\" for more details.\n",
        "        value: Value embeddings of shape :math:`(S, N, E_v)` when ``batch_first=False`` or :math:`(N, S, E_v)` when\n",
        "            ``batch_first=True``, where :math:`S` is the source sequence length, :math:`N` is the batch size, and\n",
        "            :math:`E_v` is the value embedding dimension ``vdim``. See \"Attention Is All You Need\" for more details.\n",
        "        key_padding_mask: If specified, a mask of shape :math:`(N, S)` indicating which elements within ``key``\n",
        "            to ignore for the purpose of attention (i.e. treat as \"padding\"). Binary and byte masks are supported.\n",
        "            For a binary mask, a ``True`` value indicates that the corresponding ``key`` value will be ignored for\n",
        "            the purpose of attention. For a byte mask, a non-zero value indicates that the corresponding ``key``\n",
        "            value will be ignored.\n",
        "        need_weights: If specified, returns ``attn_output_weights`` in addition to ``attn_outputs``.\n",
        "            Default: ``True``.\n",
        "        attn_mask: If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape\n",
        "            :math:`(L, S)` or :math:`(N\\cdot\\text{num\\_heads}, L, S)`, where :math:`N` is the batch size,\n",
        "            :math:`L` is the target sequence length, and :math:`S` is the source sequence length. A 2D mask will be\n",
        "            broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.\n",
        "            Binary, byte, and float masks are supported. For a binary mask, a ``True`` value indicates that the\n",
        "            corresponding position is not allowed to attend. For a byte mask, a non-zero value indicates that the\n",
        "            corresponding position is not allowed to attend. For a float mask, the mask values will be added to\n",
        "            the attention weight.\n",
        "    Outputs:\n",
        "        - **attn_output** - Attention outputs of shape :math:`(L, N, E)` when ``batch_first=False`` or\n",
        "          :math:`(N, L, E)` when ``batch_first=True``, where :math:`L` is the target sequence length, :math:`N` is\n",
        "          the batch size, and :math:`E` is the embedding dimension ``embed_dim``.\n",
        "        - **attn_output_weights** - Attention output weights of shape :math:`(N, L, S)`, where :math:`N` is the batch\n",
        "          size, :math:`L` is the target sequence length, and :math:`S` is the source sequence length. Only returned\n",
        "          when ``need_weights=True``.\n",
        "        \"\"\"\n",
        "        if self.batch_first:\n",
        "            query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\n",
        "        #결국  function 의 멀티헤드어텐션을 불러옴. \n",
        "        \"\"\"\n",
        "        _in_projection\n",
        "        _scaled_dot_product_attention\n",
        "        가 def 되어있으며 \n",
        "        multi_head_attention_forward\n",
        "        에서 마스크처리 및 진행을 함.\n",
        "        torch.view 를 이용하여 진행. 가중치를 받을 떄는 헤드 가중치.sum / num_head\n",
        "        \"\"\"\n",
        "        if not self._qkv_same_embed_dim:\n",
        "            attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
        "                query, key, value, self.embed_dim, self.num_heads,\n",
        "                self.in_proj_weight, self.in_proj_bias,\n",
        "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
        "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
        "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
        "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
        "                v_proj_weight=self.v_proj_weight)\n",
        "        else:\n",
        "            attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
        "                query, key, value, self.embed_dim, self.num_heads,\n",
        "                self.in_proj_weight, self.in_proj_bias,\n",
        "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
        "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
        "                attn_mask=attn_mask)\n",
        "        if self.batch_first:\n",
        "            return attn_output.transpose(1, 0), attn_output_weights\n",
        "        else:\n",
        "            return attn_output, attn_output_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c91_SXGBn7gW"
      },
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfpOucEQn7ly"
      },
      "source": [
        "class Seq2SeqTransformer(nn.Module): #for query ,answer \n",
        "    def __init__(self,\n",
        "                 vocab_size:int,\n",
        "                 num_encoder_layers: int=6,\n",
        "                 num_decoder_layers: int=6,\n",
        "                 emb_size: int=512,\n",
        "                 nhead: int=8,\n",
        "                 dim_feedforward: int = 2048,\n",
        "                 dropout: float = 0.1,\n",
        "                 max_len:int =40,\n",
        "                 device = None):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = TransformerModel(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout,\n",
        "                                       max_len = max_len,\n",
        "                                       device=device\n",
        "                                       )\n",
        "        self.generator = nn.Linear(emb_size, vocab_size)\n",
        "        self.tok_emb = TokenEmbedding(vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout,max_len=max_len)\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                tgt: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor,\n",
        "                max_len:int = 40\n",
        "                ):\n",
        "        print(\"check before\", src.shape)\n",
        "        src_emb = self.tok_emb(src)\n",
        "        print(\"check this\", src_emb.shape)\n",
        "        src_emb = self.positional_encoding(src_emb)\n",
        "        \n",
        "        tgt_emb = self.tok_emb(tgt)\n",
        "        tgt_emb = self.positional_encoding(tgt_emb)\n",
        "        outs = self.transformer(src_emb, tgt_emb, #src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv1ZxpGBtr8H"
      },
      "source": [
        "test_model = TransformerModel(d_model=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyp7lgARtr-4"
      },
      "source": [
        "test =torch.Tensor([[0,2,3,5,0,0,0,0],[3,5,4,6,0,0,0,0],[5,6,4,1,0,0,0,0],[9,3,3,4,0,0,0,0],[0,0,0,0,0,0,0,0]]).unsqueeze(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTEAQKI6dPLr",
        "outputId": "30d979cd-c7ce-4e55-cfb3-ee7a196da5bd"
      },
      "source": [
        "test.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca0XA7f5VjOo",
        "outputId": "17d84833-7c31-4571-c588-7329e580ec8b"
      },
      "source": [
        "test_model(test,test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.8262,  1.1515,  0.2873, -0.9334,  0.4325,  0.1636, -0.6146,\n",
              "           1.3392]],\n",
              "\n",
              "        [[-1.6673, -0.0224, -0.3781, -0.8059, -0.3237,  1.1216,  0.3635,\n",
              "           1.7123]],\n",
              "\n",
              "        [[-0.3568,  1.4376, -0.0585, -1.5922, -0.1742,  0.5598, -1.1133,\n",
              "           1.2977]],\n",
              "\n",
              "        [[-0.3376,  0.4491, -1.1459,  0.1303, -0.5496,  0.3614, -1.0857,\n",
              "           2.1778]],\n",
              "\n",
              "        [[-0.3882,  0.4722,  0.4018,  0.2850, -1.5328,  0.3487, -1.3469,\n",
              "           1.7602]]], grad_fn=<NativeLayerNormBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfS4N_iFQoH-"
      },
      "source": [
        "데이터 로드 및 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9XRd13zO6Ex"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9maKAvG8yzz"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import csv\n",
        "down_url = 'https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv'\n",
        "r = requests.get(down_url)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHFYlmfRcb5V",
        "outputId": "446eadc0-da11-42bd-fb87-0748f8783b47"
      },
      "source": [
        "open('/content/data.csv', 'wb').write(r.content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3611095"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SYoDg3W8y5w"
      },
      "source": [
        "data = pd.read_html('/content/data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQxxgtT68y_t"
      },
      "source": [
        "data_change = data[0][1].apply(lambda x : np.array(x.split(',')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG7G1cmXTBTa"
      },
      "source": [
        "data_test = pd.DataFrame([[data_change[i][0],data_change[i][1],data_change[i][2]] for i in range(1,data_change.count())],columns=data_change[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX9nEZoC_TKc"
      },
      "source": [
        "data_frame = pd.DataFrame([[data_change.values[1:][i][j] for j in range(0,3)] for i in range(0,data_change.count()-1)],columns=data_change.values[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRus8lrp_TPk"
      },
      "source": [
        "src_base = data_frame['Q']\n",
        "tgt_base = data_frame['A']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkb02LZBl180",
        "outputId": "dff6b243-bd02-4da7-b805-4acbefa1a7f2"
      },
      "source": [
        "type(src_base)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RytgnL0K_TTs"
      },
      "source": [
        "def preprocess_sentence(sentence):\n",
        "\n",
        "    sentence = re.sub(r\"([?.!,])\", \" \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "556DucJI_TY1"
      },
      "source": [
        "src_prepro = src_base.apply(preprocess_sentence)\n",
        "tgt_prepro = tgt_base.apply(preprocess_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBr8eo1n_Thj",
        "outputId": "29b89b38-7c1c-4289-e65e-3da5fc92260d"
      },
      "source": [
        "src_prepro"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                         12시 땡\n",
              "1                   1지망 학교 떨어졌어\n",
              "2                  3박4일 놀러가고 싶다\n",
              "3               3박4일 정도 놀러가고 싶다\n",
              "4                       PPL 심하네\n",
              "                  ...          \n",
              "11818             훔쳐보는 것도 눈치 보임\n",
              "11819             훔쳐보는 것도 눈치 보임\n",
              "11820                흑기사 해주는 짝남\n",
              "11821    힘든 연애 좋은 연애라는게 무슨 차이일까\n",
              "11822                힘들어서 결혼할까봐\n",
              "Name: Q, Length: 11823, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDZp2fVHnKNE"
      },
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETvA9RianKQf"
      },
      "source": [
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(src_prepro + tgt_prepro, target_vocab_size=2**13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upJ4tin4nKT5"
      },
      "source": [
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw5sA54Fn7Qs",
        "outputId": "9a8d3295-0aa9-41cc-a57f-5cfef8c249b5"
      },
      "source": [
        "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "END_TOKEN의 번호 : [8476]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATPdfpaJn7Uc"
      },
      "source": [
        "VOCAB_SIZE = tokenizer.vocab_size + 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_trkBejn7Xy"
      },
      "source": [
        "MAX_LENGTH = 40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAc-dNELn7al"
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "  mask_inputs, mask_outputs = [], []\n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "    mask_1 = np.concatenate([np.zeros(len(sentence1),dtype=np.bool),np.ones(MAX_LENGTH-len(sentence1),dtype=np.bool)])\n",
        "    mask_2 = np.concatenate([np.zeros(len(sentence2),dtype=np.bool),np.ones(MAX_LENGTH-len(sentence2),dtype=np.bool)])\n",
        "\n",
        "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "      tokenized_inputs.append(sentence1)\n",
        "      tokenized_outputs.append(sentence2)\n",
        "      mask_inputs.append(mask_1)\n",
        "      mask_outputs.append(mask_2)\n",
        "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  \n",
        "\n",
        "  return tokenized_inputs, np.array(mask_inputs), tokenized_outputs, np.array(mask_outputs,dtype= np.bool)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBG9D95dn7dP"
      },
      "source": [
        "filtered_data = tokenize_and_filter(src_prepro,tgt_prepro)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjro8REqGpv6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LJJHARFEPu_"
      },
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data[0])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[0][idx],self.data[1][idx],self.data[2][idx],self.data[3][idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyY0SXIGFJJn"
      },
      "source": [
        "my_data = MyDataset(filtered_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c1irRB3Gy2r",
        "outputId": "6bb777db-2b4c-43ef-927d-65418a0bf9df"
      },
      "source": [
        "len(my_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11823"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia3SZyIsmTSr"
      },
      "source": [
        "파라미터 설정 및 모델 train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y00BT17BrJq7"
      },
      "source": [
        " def customize_LR(epoch,d_model,warmup_steps):\n",
        "    epoch, warmup_steps = torch.Tensor([epoch]),torch.Tensor([warmup_steps])\n",
        "    arg1 = torch.rsqrt(epoch)\n",
        "    arg2 = epoch * (warmup_steps**-1.5)\n",
        "    return d_model * torch.minimum(arg1, arg2).item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1igrhp81YRIr",
        "outputId": "eaa5ddf2-49d7-4e57-e74f-c67087dea1d4"
      },
      "source": [
        "type(torch.Tensor([1]).item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "float"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErgdPrxsn7ob"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "#VOCAB_SIZE = VOCAB_SIZE # tokenizer.vocab_size + 2\n",
        "#EMB_SIZE = 512\n",
        "#NHEAD = 8\n",
        "#FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "#NUM_ENCODER_LAYERS = 3\n",
        "#NUM_DECODER_LAYERS = 3\n",
        "PAD_IDX = 0\n",
        "\n",
        "transformer = Seq2SeqTransformer(VOCAB_SIZE,max_len=40,device = device)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "lambda1 = lambda epoch: customize_LR(epoch,d_model=512,warmup_steps=4000)\n",
        "#scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWDesYaTv28z"
      },
      "source": [
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_dataloader = DataLoader(my_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "    for src,src_padding_mask, tgt,tgt_padding_mask in train_dataloader:\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "        \n",
        "        src = src.transpose(0,1)\n",
        "        tgt = tgt.transpose(0,1)\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        \n",
        "        print(\"check\",src.shape,\"test\",tgt_input.shape)\n",
        "        logits = model(src, tgt,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits[:-1].reshape(-1, logits.shape[-1]), tgt_out.reshape(-1).long())\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "    scheduler.step()\n",
        "\n",
        "    return losses / len(train_dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_dataloader = DataLoader(filtered_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "    for src,src_padding_mask, tgt,tgt_padding_mask in val_dataloader:\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "        src_padding_mask = src_padding_mask.to(device)\n",
        "        tgt_padding_mask = tgt_padding_mask.to(device)\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src = torch.transpose(src,0,1)\n",
        "        tgt_input=torch.transpose(tgt_input,0,1)\n",
        "        logits = model(src, tgt,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(val_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDOL7k0xCekB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f2be4753-bdc0-4e08-c592-24ae3f9d516a"
      },
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla V100-SXM2-16GB'"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "yji-4_rC9hWc",
        "outputId": "62396261-a0d7-49de-c83a-bd45dad2b71c"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check torch.Size([40, 128]) test torch.Size([39, 128])\n",
            "check before torch.Size([40, 128])\n",
            "check this torch.Size([40, 128, 512])\n",
            "torch.Size([40, 1, 512])  test torch.Size([40, 128, 512])\n",
            "torch.Size([40, 1, 512])  test torch.Size([40, 128, 512])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-155-688719a42214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-153-87d648d84c16>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"check\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-123-062d565e8b4d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_padding_mask, tgt_padding_mask, memory_key_padding_mask, max_len)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtgt_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         outs = self.transformer(src_emb, tgt_emb, #src_mask, tgt_mask, None,\n\u001b[0;32m---> 42\u001b[0;31m                                 src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-115-5d40e8a2e30e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, max_len)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n\u001b[1;32m    105\u001b[0m                               \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-116-293c6ed3e8e9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-118-f2e5e9da7cea>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-118-f2e5e9da7cea>\u001b[0m in \u001b[0;36m_sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m     64\u001b[0m                            \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                            \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                            need_weights=False)[0]\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-121-28ef308c74e5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   5099\u001b[0m     \u001b[0;31m# (deep breath) calculate attention and out projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5100\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5101\u001b[0;31m     \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_scaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5102\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5103\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_scaled_dot_product_attention\u001b[0;34m(q, k, v, attn_mask, dropout_p)\u001b[0m\n\u001b[1;32m   4844\u001b[0m     \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mattn_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4846\u001b[0;31m         \u001b[0mattn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4847\u001b[0m     \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4848\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdropout_p\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od5ksuRuv3RD"
      },
      "source": [
        "\n",
        "\n",
        "# 탐욕(greedy) 알고리즘을 사용하여 출력 순서(sequence)를 생성하는 함수\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(device)\n",
        "    src_mask = src_mask.to(device)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(device)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(device)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# 입력 문장을 도착어로 번역하는 함수\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvyb5Dqd7IAm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbpRylCX7IGf"
      },
      "source": [
        "torch.Tensor([[1,2,3]]).transpose(0,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dOugZnU7IIQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}