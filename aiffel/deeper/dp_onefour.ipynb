{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dp_onefour.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPdvbX/vVw8SDwbv0WgEwJx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kim1987/aiffel/blob/main/aiffel/deeper/dp_onefour.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "deeper_14\n"
      ],
      "metadata": {
        "id": "tQ0K5vH-vC_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyNVHBeJaT4G",
        "outputId": "8ad154ef-9375-484c-f78a-add059a1aa2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/colabdata/aiffel/dp_onefour/kowiki.txt.zip /content"
      ],
      "metadata": {
        "id": "AY2DZstDaKlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/kowiki.txt.zip -d /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFRxPPXNcC4v",
        "outputId": "0ef33277-8235-49ad-919e-abbf837a62e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/kowiki.txt.zip\n",
            "replace /content/kowiki.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE0koG6TClQy",
        "outputId": "e569eecf-e49a-4be2-c36e-e1a21c79a193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiMh8z-755sC",
        "outputId": "06ffffd3-abee-4197-df20-f7bae4362b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "import os\n",
        "import shutil"
      ],
      "metadata": {
        "id": "_4JPILC8cC-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel,BertConfig"
      ],
      "metadata": {
        "id": "mFpXvsiM2Ezc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLNetTokenizer\n",
        "import transformers"
      ],
      "metadata": {
        "id": "0NX9rjvED4PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "2S_1Z0beIC1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 전처리"
      ],
      "metadata": {
        "id": "DjMMexT8k22v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_path = \"/content/kowiki.txt\"\n",
        "os_path = '/content/'\n",
        "drive_path = '/content/drive/MyDrive/colabdata/aiffel/dp_onefour/'"
      ],
      "metadata": {
        "id": "D21EGTWwgNQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = 'ko_32000'\n",
        "vocab_size = 32000\n",
        "max_len = 60"
      ],
      "metadata": {
        "id": "xCrwV7CRDobb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(not os.path.exists(os_path+prefix+'.model')) and os.path.exists(drive_path+prefix+'.model'):\n",
        "  shutil.copyfile(drive_path+prefix+'.model',os_path+prefix+'.model')\n",
        "  shutil.copyfile(drive_path+prefix+'.vocab',os_path+prefix+'.vocab')"
      ],
      "metadata": {
        "id": "P_aACjE6DSOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(os_path+prefix+'.model'):\n",
        "  spm.SentencePieceTrainer.train(\n",
        "      f\"--input={my_path} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
        "      \" --model_type=bpe\" +\n",
        "      \" --max_sentence_length=999999\" +\n",
        "      \" --pad_id=0 --pad_piece=[PAD]\" +\n",
        "      \" --unk_id=1 --unk_piece=[UNK]\" +\n",
        "      \" --bos_id=2 --bos_piece=[BOS]\" +\n",
        "      \" --eos_id=3 --eos_piece=[EOS]\" +\n",
        "      \" --user_defined_symbols=[SEP],[CLS],[MASK]\")"
      ],
      "metadata": {
        "id": "JU5lQShk-Ay0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(drive_path+prefix+'.model'):\n",
        "  shutil.copyfile(os_path+prefix+'.model',drive_path+prefix+'.model')\n",
        "  shutil.copyfile(os_path+prefix+'.vocab',drive_path+prefix+'.vocab')"
      ],
      "metadata": {
        "id": "olG5BUiMVDrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tokenizer = XLNetTokenizer(os_path+prefix+'.model',\n",
        "                              do_lower_case = False,\n",
        "                              remove_space = True,\n",
        "                              bos_token = '[BOS]',\n",
        "                              eos_token = '[EOS]',\n",
        "                              unk_token = '[UNK]',\n",
        "                              sep_token = '[SEP]',\n",
        "                              pad_token = '[PAD]',\n",
        "                              cls_token = '[CLS]',\n",
        "                              mask_token = '[MASK]')"
      ],
      "metadata": {
        "id": "OIBTZR-7D0XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tokenizer.padding_side='right'"
      ],
      "metadata": {
        "id": "UGl4MNvoZ8__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tokenizer('이것은','다른가',padding='max_length',truncation='longest_first',max_length=max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_jD0nLEYYzg",
        "outputId": "f2df3a15-d0e2-47fa-c705-e73a64ca3831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [1487, 4, 218, 27608, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 1, 1, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], 'attention_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_mask(word,vocab_size):\n",
        "  if torch.rand(1)<=0.8:\n",
        "    return '[MASK]'\n",
        "  elif torch.rand(1) <= 0.5:\n",
        "    return my_tokenizer.decode(torch.randint(0,vocab_size+7,1))\n",
        "  return word\n",
        "  \n",
        "def masking_sentence(words):\n",
        "  return [word  if value<=0.85 else \"[MASK]\" for word,value in zip(words,torch.rand(len(words)))]"
      ],
      "metadata": {
        "id": "hHbJ-EKgjWD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trans_subset(sen1,sen2,tokenizer):\n",
        "  is_next = True\n",
        "  if torch.rand(1)>=0.5:\n",
        "    sen1,sen2 = sen2,sen1\n",
        "    is_next = False\n",
        "\n",
        "  masked_sen1 = \" \".join(masking_sentence(sen1.split(\" \")))\n",
        "  masked_sen2 = \" \".join(masking_sentence(sen2.split(\" \")))\n",
        "  \n",
        "  tokened_orig = my_tokenizer(sen1,sen2,padding='max_length',truncation='longest_first',max_length=max_len)\n",
        "  tokened_mask = my_tokenizer(masked_sen1,sen2,padding='max_length',truncation='longest_first',max_length=max_len)\n",
        "\n",
        "  return tokened_mask.input_ids,tokened_mask.token_type_ids, tokened_mask.attention_mask, tokened_orig.input_ids,is_next"
      ],
      "metadata": {
        "id": "-zUcZTKrakIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(os_path+'sentences.txt'):\n",
        "  if not os.path.exists(drive_path+'sentences.txt'):\n",
        "    with open(my_path,'r') as f:\n",
        "      with open(os_path+'sentences.txt','w') as fw:\n",
        "        is_new=False\n",
        "        for line in f:\n",
        "          if line =='\\n':\n",
        "            is_new=False\n",
        "          else:\n",
        "            if is_new:\n",
        "              fw.write(line.rstrip('\\n') + ' ')\n",
        "            else:\n",
        "              if is_new == False:\n",
        "                is_new=True\n",
        "                fw.write('\\n')\n",
        "      shutil.copyfile(os_path+'sentences.txt',drive_path+'sentences.txt')\n",
        "  else:\n",
        "    shutil.copyfile(drive_path+'sentences.txt',os_path+'sentences.txt')"
      ],
      "metadata": {
        "id": "cV8swGH4oAJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not (os.path.exists(os_path+'pairs.txt') and os.path.exists(drive_path+'pairs.txt')):\n",
        "  with open(os_path+'sentences.txt','r') as f:\n",
        "    with open(os_path+'pairs.txt','w') as fw:\n",
        "      for strings in f:\n",
        "          sep_strings = strings.split(\". \")\n",
        "          for str1,str2 in zip(sep_strings[:-2],sep_strings[1:-1]):\n",
        "            fw.write(str1+'.'+str2+'\\n')\n",
        "    shutil.copyfile(os_path+'pairs.txt',drive_path+'pairs.txt')\n",
        "else:\n",
        "  shutil.copyfile(drive_path+'pairs.txt',os_path+'pairs.txt')"
      ],
      "metadata": {
        "id": "1yZssTCwEHgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os_path+'pairs.txt','r') as fw:\n",
        "  my_data =pd.DataFrame([sen for sen in fw])"
      ],
      "metadata": {
        "id": "1ePd6CVNeVOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_data = my_data.applymap(lambda x: x.split('.'))"
      ],
      "metadata": {
        "id": "CfxFyvn2qO3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_data"
      ],
      "metadata": {
        "id": "QyPvuazqrO2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self):\n",
        "    self.data = my_data.to_numpy()\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return trans_subset(self.data[idx][0][0]+'.',self.data[idx][0][1].rstrip('\\n')+'.',my_tokenizer)"
      ],
      "metadata": {
        "id": "dppuVNOgPdAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dataset = MyDataset()\n",
        "my_loader = torch.utils.data.DataLoader(my_dataset)"
      ],
      "metadata": {
        "id": "VIsSyhcWSw1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqP-3DKtqcqw",
        "outputId": "82f45676-08ed-4927-9b5f-e3c652e379d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([273,\n",
              "  27599,\n",
              "  4,\n",
              "  30,\n",
              "  27599,\n",
              "  4,\n",
              "  5,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  2,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3],\n",
              " [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [273,\n",
              "  27599,\n",
              "  4,\n",
              "  30,\n",
              "  27599,\n",
              "  4,\n",
              "  5,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " False)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_configure = BertConfig(vocab_size = vocab_size+7)\n",
        "my_bert = BertModel(bert_configure)"
      ],
      "metadata": {
        "id": "VRS7OjpxVpwZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}